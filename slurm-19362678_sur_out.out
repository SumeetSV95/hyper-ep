GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
SLURM auto-requeueing enabled. Setting signal handlers.
Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/13 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/13 [00:00<?, ?it/s]/home/sv6234/miniconda3/envs/miccai/lib/python3.11/site-packages/torch/nn/modules/conv.py:309: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv1d(input, weight, bias, self.stride,
torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0:   8%|▊         | 1/13 [01:00<12:05,  0.02it/s]torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0:  15%|█▌        | 2/13 [02:15<12:22,  0.01it/s]torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0:  23%|██▎       | 3/13 [03:21<11:13,  0.01it/s]torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0:  31%|███       | 4/13 [04:27<10:01,  0.01it/s]torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0:  38%|███▊      | 5/13 [05:31<08:50,  0.02it/s]torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0:  46%|████▌     | 6/13 [06:34<07:40,  0.02it/s]torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0:  54%|█████▍    | 7/13 [07:38<06:32,  0.02it/s]torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0:  62%|██████▏   | 8/13 [08:41<05:25,  0.02it/s]torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0:  69%|██████▉   | 9/13 [09:44<04:19,  0.02it/s]torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0:  77%|███████▋  | 10/13 [10:45<03:13,  0.02it/s]torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0:  85%|████████▍ | 11/13 [11:47<02:08,  0.02it/s]torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0:  92%|█████████▏| 12/13 [12:50<01:04,  0.02it/s]torch.Size([16, 2238]) torch.Size([16, 117, 1119]) torch.Size([16, 10, 117, 1119]) torch.Size([16])
Testing DataLoader 0: 100%|██████████| 13/13 [13:53<00:00,  0.02it/s]Testing DataLoader 0: 100%|██████████| 13/13 [13:53<00:00,  0.02it/s]
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
0.07999999821186066_mean    0.12796101667560064
 0.07999999821186066_std   0.0002751782109519472
      0.08_act_loss         2.7740795612335205
      0.08_act_std           3.965169668197632
     0.08_param_loss       0.0023003730457276106
 0.08_tmp_loss_internal    0.019467364996671677
  0.08_tmp_loss_surface    0.019939230754971504
  0.08_tmp_std_internal     0.11419491469860077
  0.08_tmp_std_surface      0.1162230521440506
0.10000000149011612_mean    0.13652237344246643
 0.10000000149011612_std  0.00013873796813753308
0.11999999731779099_mean    0.14679546661866016
 0.11999999731779099_std   0.0003796435826489881
      0.12_act_loss         0.8901284337043762
      0.12_act_std          1.0485365390777588
     0.12_param_loss       0.0007182701956480742
 0.12_tmp_loss_internal    0.004399852827191353
  0.12_tmp_loss_surface     0.00469077518209815
  0.12_tmp_std_internal     0.06613244861364365
  0.12_tmp_std_surface      0.0681595727801323
0.14000000059604645_mean    0.15800445836005558
 0.14000000059604645_std   0.0016867345041100541
      0.14_act_loss         1.2819652557373047
      0.14_act_std           1.541485071182251
     0.14_param_loss       0.0003284715348854661
 0.14_tmp_loss_internal    0.005761643406003714
  0.14_tmp_loss_surface    0.005818020086735487
  0.14_tmp_std_internal     0.07424326241016388
  0.14_tmp_std_surface      0.0748063400387764
      0.1_act_loss          0.6319275498390198
       0.1_act_std          0.8466230034828186
     0.1_param_loss        0.0013339101569727063
  0.1_tmp_loss_internal    0.005336505360901356
  0.1_tmp_loss_surface     0.005811632145196199
  0.1_tmp_std_internal      0.0711427703499794
   0.1_tmp_std_surface      0.07413800060749054
           act               4.333066940307617
         act_std             7.420431137084961
           end              1.4793256521224976
         end_std            3.0889880657196045
      internal_loss        0.009217820130288601
       param_loss          0.0011715267319232225
          start             3.2162129878997803
        start_std            5.893956184387207
           std              0.08997812867164612
        std_param           0.01148910354822874
      surface_loss         0.009505381807684898
          test              0.00934040080755949
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Main Table:
                     Value
test        tensor(0.0093)
param_loss  tensor(0.0012)
std         tensor(0.0900)
std_param   tensor(0.0115)
act         tensor(4.3331)
start       tensor(3.2162)
end         tensor(1.4793)
act_std     tensor(7.4204)
start_std   tensor(5.8940)
end_std     tensor(3.0890)

Activation Length Loss Table:
                        Value
0.12_act_loss  tensor(0.8901)
0.14_act_loss  tensor(1.2820)
0.08_act_loss  tensor(2.7741)
0.1_act_loss   tensor(0.6319)

Parameter Loss Table:
                          Value
0.12_param_loss  tensor(0.0007)
0.14_param_loss  tensor(0.0003)
0.08_param_loss  tensor(0.0023)
0.1_param_loss   tensor(0.0013)
